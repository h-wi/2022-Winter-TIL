{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/h-wi/2022-Winter-TIL/blob/main/TFLite_Model_Maker_Object_Detection_ipynb%EC%9D%98_%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Environment setup"
      ],
      "metadata": {
        "id": "PtwI7XFUp-qx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt -y install libportaudio2\n",
        "!pip install -q --use-deprecated=legacy-resolver tflite-model-maker\n",
        "!pip install -q pycocotools\n",
        "!pip install -q opencv-python-headless==4.1.2.30\n",
        "!pip uninstall -y tensorflow && pip install -q tensorflow==2.8.0"
      ],
      "metadata": {
        "id": "5GJQx_BsqBXx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc247a87-90ad-4256-b0dd-e846b02c6cd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  libportaudio2\n",
            "0 upgraded, 1 newly installed, 0 to remove and 27 not upgraded.\n",
            "Need to get 65.4 kB of archives.\n",
            "After this operation, 223 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 libportaudio2 amd64 19.6.0-1build1 [65.4 kB]\n",
            "Fetched 65.4 kB in 0s (174 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libportaudio2:amd64.\n",
            "(Reading database ... 129499 files and directories currently installed.)\n",
            "Preparing to unpack .../libportaudio2_19.6.0-1build1_amd64.deb ...\n",
            "Unpacking libportaudio2:amd64 (19.6.0-1build1) ...\n",
            "Setting up libportaudio2:amd64 (19.6.0-1build1) ...\n",
            "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m577.3/577.3 KB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.5/77.5 KB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.6/128.6 KB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m95.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m108.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 KB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.9/840.9 KB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.9/238.9 KB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 KB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.3/25.3 MB\u001b[0m \u001b[31m76.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.4/222.4 KB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m352.1/352.1 KB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's legacy dependency resolver does not consider dependency conflicts when selecting packages. This behaviour is the source of the following dependency conflicts.\n",
            "xarray 2022.12.0 requires packaging>=21.3, but you'll have packaging 20.9 which is incompatible.\n",
            "tensorflow 2.9.2 requires flatbuffers<2,>=1.12, but you'll have flatbuffers 23.1.21 which is incompatible.\n",
            "scann 1.2.6 requires tensorflow~=2.8.0, but you'll have tensorflow 2.9.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.8/21.8 MB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hFound existing installation: tensorflow 2.9.2\n",
            "Uninstalling tensorflow-2.9.2:\n",
            "  Successfully uninstalled tensorflow-2.9.2\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m497.6/497.6 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m85.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.5/462.5 KB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import libraries"
      ],
      "metadata": {
        "id": "Gimmv5HFq3cX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3maVzk2WsoRZ",
        "outputId": "a1e39a27-cec0-4432-dd1d-b130092ec32d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from tflite_model_maker.config import QuantizationConfig\n",
        "from tflite_model_maker.config import ExportFormat\n",
        "from tflite_model_maker import model_spec\n",
        "from tflite_model_maker import object_detector\n",
        "\n",
        "import tensorflow as tf\n",
        "assert tf.__version__.startswith('2')\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "from absl import logging\n",
        "logging.set_verbosity(logging.ERROR)\n",
        "\n",
        "tf.test.is_gpu_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SX7Gb3Naq215",
        "outputId": "cd778c6f-ad87-4a49-ffbc-14ce3d5a3715"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.9.0 and strictly below 2.12.0 (nightly versions are not supported). \n",
            " The versions of TensorFlow you are currently using is 2.8.0 and is not supported. \n",
            "Some things might work, some things might not.\n",
            "If you were to encounter a bug, do not file an issue.\n",
            "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
            "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
            "https://github.com/tensorflow/addons\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xvf /content/drive/MyDrive/plastic_bottles.tar"
      ],
      "metadata": {
        "id": "DI24al9jrZVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/Annotations | head -5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4ylKwKRsCVG",
        "outputId": "ec4ba745-4bac-478f-9acb-c9af03d9cd7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2001_jpg.rf.ba1857508348a8dae58dec77f018109d.xml\n",
            "2002_jpg.rf.1ae8d5564ef53bfe1f56b5249db47cbf.xml\n",
            "2003_jpg.rf.0490359686436e0dc62675ec8dd2f848.xml\n",
            "2004_jpg.rf.992323f4a0714cada84705b35eb579b1.xml\n",
            "2005_jpg.rf.cfaa3e66fd8168a7eabe6d9c80795850.xml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "import cv2\n",
        "\n",
        "\n",
        "def read_content(xml_file: str):\n",
        "    tree = ET.parse(xml_file)\n",
        "    root = tree.getroot()\n",
        "    filename = root.find('filename').text\n",
        "    filename = os.path.join(\"/content/Images\", filename)\n",
        "    image = cv2.imread(filename)\n",
        "    h, w = image.shape[:2]\n",
        "\n",
        "    labels = []\n",
        "    for boxes in root.iter('object'):\n",
        "        ymin, xmin, ymax, xmax = None, None, None, None\n",
        "        class_name = boxes.find(\"name\").text\n",
        "        ymin = int(boxes.find(\"bndbox/ymin\").text) / h\n",
        "        xmin = int(boxes.find(\"bndbox/xmin\").text) / w\n",
        "        ymax = int(boxes.find(\"bndbox/ymax\").text) / h\n",
        "        xmax = int(boxes.find(\"bndbox/xmax\").text) / w\n",
        "        list_with_single_boxes = [xmin, ymin, xmax, ymax]\n",
        "        labels.append({class_name: list_with_single_boxes})\n",
        "    return filename, labels\n",
        "\n",
        "filename, labels = read_content(\"/content/Annotations/2001_jpg.rf.ba1857508348a8dae58dec77f018109d.xml\")"
      ],
      "metadata": {
        "id": "bX5BxwO-rjfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ClVx7Msc_oRK",
        "outputId": "77f9b0c1-45b1-4a05-be3e-793d8b7a04df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/Images/2001_jpg.rf.ba1857508348a8dae58dec77f018109d.jpg'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "313aDYgdBxca",
        "outputId": "72c1e98e-b901-4953-ad1c-9bf88b31ea55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'lid': [0.5048076923076923,\n",
              "   0.20673076923076922,\n",
              "   0.5480769230769231,\n",
              "   0.24759615384615385]},\n",
              " {'label': [0.4951923076923077,\n",
              "   0.2860576923076923,\n",
              "   0.5865384615384616,\n",
              "   0.4495192307692308]}]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "\n",
        "annotations = glob.glob(\"/content/Annotations/*\")\n",
        "\n",
        "with open(\"voc.csv\", \"w\") as f:\n",
        "    for i, path in enumerate(annotations):\n",
        "        image_path, labels = read_content(path)\n",
        "\n",
        "        seventy_percent = int(len(annotations) * 0.7)\n",
        "\n",
        "        if i < seventy_percent:\n",
        "            dataset_fold = \"TRAIN\"\n",
        "        elif i > seventy_percent < seventy_percent+500:\n",
        "            dataset_fold = \"TEST\"\n",
        "        else:\n",
        "            dataset_fold = \"VALIDATE\"\n",
        "\n",
        "        for obj in labels:\n",
        "            class_name, (x1,y1,x2,y2) = list(obj.items())[0]\n",
        "            label = f\"{dataset_fold},{image_path},{class_name},{x1},{y1},,,{x2},{y2},,\"\n",
        "            f.write(label+\"\\n\")"
      ],
      "metadata": {
        "id": "kMMAChvlt5ZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model preparation"
      ],
      "metadata": {
        "id": "0oXsh388xJPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spec = model_spec.get('efficientdet_lite3')\n",
        "train_data, validation_data, test_data = object_detector.DataLoader.from_csv('voc.csv')"
      ],
      "metadata": {
        "id": "pOk8sfZkw3Q3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model training"
      ],
      "metadata": {
        "id": "RXT7v1kcxVHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = object_detector.create(train_data, model_spec=spec, batch_size=16, epochs=20, train_whole_model=True, validation_data=validation_data)"
      ],
      "metadata": {
        "id": "3iZnYWUGxUp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.export(export_dir='./')"
      ],
      "metadata": {
        "id": "tpRbsb85yY2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### inference via tensorflow"
      ],
      "metadata": {
        "id": "I30UJshm1oHO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from PIL import Image\n",
        "import time\n",
        "import glob\n",
        "\n",
        "model_path = 'model.tflite'\n",
        "\n",
        "# Load the labels into a list\n",
        "classes = ['???'] * model.model_spec.config.num_classes\n",
        "label_map = model.model_spec.config.label_map\n",
        "\n",
        "for label_id, label_name in label_map.as_dict().items():\n",
        "    classes[label_id-1] = label_name\n",
        "\n",
        "# Define a list of colors for visualization\n",
        "COLORS = np.random.randint(0, 255, size=(len(classes), 3), dtype=np.uint8)\n",
        "\n",
        "def preprocess_image(image_path, input_size):\n",
        "    \"\"\"Preprocess the input image to feed to the TFLite model\"\"\"\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.io.decode_image(img, channels=3)\n",
        "    img = tf.image.convert_image_dtype(img, tf.uint8)\n",
        "    original_image = img\n",
        "    resized_img = tf.image.resize(img, input_size)\n",
        "    resized_img = resized_img[tf.newaxis, :]\n",
        "    resized_img = tf.cast(resized_img, dtype=tf.uint8)\n",
        "    return resized_img, original_image\n",
        "\n",
        "\n",
        "def detect_objects(interpreter, image, threshold):\n",
        "    \"\"\"Returns a list of detection results, each a dictionary of object info.\"\"\"\n",
        "\n",
        "    signature_fn = interpreter.get_signature_runner()\n",
        "\n",
        "    # Feed the input image to the model\n",
        "    st = time.time()\n",
        "    output = signature_fn(images=image)\n",
        "    print(f\"Elapsed: {(time.time() - st)*1000:.3f} ms\")\n",
        "    \n",
        "\n",
        "    # Get all outputs from the model\n",
        "    count = int(np.squeeze(output['output_0']))\n",
        "    scores = np.squeeze(output['output_1'])\n",
        "    classes = np.squeeze(output['output_2'])\n",
        "    boxes = np.squeeze(output['output_3'])\n",
        "\n",
        "    results = []\n",
        "    for i in range(count):\n",
        "        if scores[i] >= threshold:\n",
        "            result = {\n",
        "            'bounding_box': boxes[i],\n",
        "            'class_id': classes[i],\n",
        "            'score': scores[i]\n",
        "            }\n",
        "            results.append(result)\n",
        "    return results\n",
        "\n",
        "\n",
        "def run_odt_and_draw_results(image_path, interpreter, threshold=0.5):\n",
        "    \"\"\"Run object detection on the input image and draw the detection results\"\"\"\n",
        "    # Load the input shape required by the model\n",
        "    _, input_height, input_width, _ = interpreter.get_input_details()[0]['shape']\n",
        "\n",
        "    # Load the input image and preprocess it\n",
        "    preprocessed_image, original_image = preprocess_image(\n",
        "      image_path,\n",
        "      (input_height, input_width)\n",
        "    )\n",
        "\n",
        "    # Run object detection on the input image\n",
        "    results = detect_objects(interpreter, preprocessed_image, threshold=threshold)\n",
        "\n",
        "    # Plot the detection results on the input image\n",
        "    original_image_np = original_image.numpy().astype(np.uint8)\n",
        "    for obj in results:\n",
        "        # Convert the object bounding box from relative coordinates to absolute\n",
        "        # coordinates based on the original image resolution\n",
        "        ymin, xmin, ymax, xmax = obj['bounding_box']\n",
        "        xmin = int(xmin * original_image_np.shape[1])\n",
        "        xmax = int(xmax * original_image_np.shape[1])\n",
        "        ymin = int(ymin * original_image_np.shape[0])\n",
        "        ymax = int(ymax * original_image_np.shape[0])\n",
        "\n",
        "        # Find the class index of the current object\n",
        "        class_id = int(obj['class_id'])\n",
        "\n",
        "        # Draw the bounding box and label on the image\n",
        "        color = [int(c) for c in COLORS[class_id]]\n",
        "        cv2.rectangle(original_image_np, (xmin, ymin), (xmax, ymax), color, 2)\n",
        "        # Make adjustments to make the label visible for all objects\n",
        "        y = ymin - 15 if ymin - 15 > 15 else ymin + 15\n",
        "        label = \"{}: {:.0f}%\".format(classes[class_id], obj['score'] * 100)\n",
        "        cv2.putText(original_image_np, label, (xmin, y),\n",
        "            cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "\n",
        "    # Return the final image\n",
        "    original_uint8 = original_image_np.astype(np.uint8)\n",
        "    return original_uint8"
      ],
      "metadata": {
        "id": "rvgnXnLo0JNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "import random\n",
        "images = glob.glob(\"/content/Images/*\")"
      ],
      "metadata": {
        "id": "amXLwpVZ04mA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the TFLite model\n",
        "interpreter = tf.lite.Interpreter(model_path=model_path)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "#Run object detection and show the detection results\n",
        "DETECTION_THRESHOLD = 0.5\n",
        "\n",
        "for i in range(5):\n",
        "    image = images[random.randint(1, len(images))]\n",
        "    \n",
        "    im = Image.open(image)\n",
        "    im.thumbnail((320, 320), Image.ANTIALIAS)\n",
        "    im.save(\"/tmp/image.png\", 'PNG')\n",
        "    \n",
        "    # Run inference and draw detection result on the local copy of the original file\n",
        "    detection_result_image = run_odt_and_draw_results(\n",
        "        \"/tmp/image.png\",\n",
        "        interpreter,\n",
        "        threshold=DETECTION_THRESHOLD\n",
        "    )\n",
        "\n",
        "    # Show the detection result\n",
        "    display(Image.fromarray(detection_result_image))"
      ],
      "metadata": {
        "id": "KcWVBpZv1MY3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}